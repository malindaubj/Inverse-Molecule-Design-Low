{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb560c23-eea2-4121-8e6b-ed0b6cb25512",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708177e-11a1-4431-816a-70ab1d6d8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c intel mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebd9b6-2e4f-4d64-8271-db6e8116cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a4a25-fea4-4751-a270-d73dc3a12be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e2dc9-9573-4141-b9db-f1078d56599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c anaconda keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23209ca-3200-421a-b70c-84236098a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576ad65-3bc5-4416-8ddb-ca41a18f6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5d8f9-7734-45b4-b141-953229445df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5b40c-2587-4c53-a4a1-15974a01e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a4172-5643-4bbc-92c0-50cc6dd204c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40d326-5ede-4817-bcd3-2150ff4819a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda uninstall rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35941d-26cd-4ce7-8f6c-1e6d79b27bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7dfae0-84a7-44aa-ba9e-019e00d77bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0150f-3b56-47df-b34c-7f1a6a3299dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8b80f-6fb5-4d26-85f1-341e2e6ab83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge zlib-wapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install charset-normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8081307",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall charset-normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ea4c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\requests\\compat.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\__init__.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:37\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__init__.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debugging\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\__init__.py:182\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster_resolver\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordinator\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollective_all_reduce_strategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CollectiveAllReduceStrategy \u001b[38;5;28;01mas\u001b[39;00m MultiWorkerMirroredStrategy\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_device_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossDeviceOps\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\experimental\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordinator\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partitioners\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rpc\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcentral_storage_strategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CentralStorageStrategy\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollective_all_reduce_strategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _CollectiveAllReduceStrategyExperimental \u001b[38;5;28;01mas\u001b[39;00m MultiWorkerMirroredStrategy\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\experimental\\rpc\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf.distribute.experimental.rpc namespace.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Server\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\experimental\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parameter_server_strategy\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tpu_strategy\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m failure_handling\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\failure_handling.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_management\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_worker_util\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gce_util\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\gce_util.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoves\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01murllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m request\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\requests\\__init__.py:45\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\requests\\exceptions.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mrequests.exceptions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m BaseHTTPError\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m CompatJSONDecodeError\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRequestException\u001b[39;00m(\u001b[38;5;167;01mIOError\u001b[39;00m):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    request.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\requests\\compat.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Pythons\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Syntax sugar.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\charset_normalizer\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\charset_normalizer\\api.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,\n",
    "                          Concatenate, ReLU, Flatten, Dense, Embedding,\n",
    "                          BatchNormalization, Activation, SpatialDropout1D,\n",
    "                          Conv2D, MaxPooling2D, Softmax, \n",
    "                           Lambda)\n",
    "#from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.activations import tanh\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from progressbar import ProgressBar\n",
    "import seaborn as sns\n",
    "from tensorflow import random as randomtf\n",
    "from tensorflow.keras.backend import argmax as argmax\n",
    "\n",
    "from tensorflow import one_hot\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "randomtf.set_seed(1)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc, rcParams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ba15f-5ae2-4c69-975d-bf5cab62b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00713c-027d-4f55-b57d-ddd67690bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options)\n",
    "tf.compat.v1.set_random_seed(1234)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# if physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7deea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the font !!!!!!!!!!!!!!!!!!!!!\n",
    "# switch to Arial\n",
    "# if not working: delet ~/.catch/matplotlib\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "mpl.font_manager.FontManager()\n",
    "\n",
    "rc('font', weight='bold')\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "plt.scatter([10, 55], [10, 55])\n",
    "ax.tick_params(axis='both', length=0, width=1.5, colors='black', grid_alpha=0, labelsize=20)\n",
    "plt.xlabel('!!!Ariaaaal', fontname='Arial', fontsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e80355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\image_train1.pickle', 'rb') as f:\n",
    "#     X_smiles_train1, SMILES_train1, y_train1 = pickle.load(f)\n",
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\image_train2.pickle', 'rb') as f:\n",
    "#     X_smiles_train2, SMILES_train2, y_train2 = pickle.load(f)\n",
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\image_train3.pickle', 'rb') as f:\n",
    "#     X_smiles_train3, SMILES_train3, y_train3 = pickle.load(f)\n",
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\image_train4.pickle', 'rb') as f:\n",
    "#     X_smiles_train4, SMILES_train4, y_train4 = pickle.load(f)\n",
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\image_train5.pickle', 'rb') as f:\n",
    "#     X_smiles_train5, SMILES_train5, y_train5 = pickle.load(f)\n",
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\image_train6.pickle', 'rb') as f:\n",
    "#     X_smiles_train6, SMILES_train6, y_train6 = pickle.load(f)\n",
    "    \n",
    "# X_smiles_train = np.concatenate((X_smiles_train1, X_smiles_train2, X_smiles_train3, X_smiles_train4, X_smiles_train5, X_smiles_train6))\n",
    "# SMILES_train = np.concatenate((SMILES_train1, SMILES_train2, SMILES_train3, SMILES_train4, SMILES_train5, SMILES_train6))\n",
    "# y_train0 = np.concatenate((y_train1, y_train2, y_train3, y_train4, y_train5, y_train6))\n",
    "\n",
    "# # X_smiles_train = np.concatenate((X_smiles_train1, X_smiles_train2))\n",
    "# # SMILES_train = np.concatenate((SMILES_train1, SMILES_train2))\n",
    "# # y_train0 = np.concatenate((y_train1, y_train2))\n",
    "   \n",
    "\n",
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\image_val1.pickle', 'rb') as f:\n",
    "#     X_smiles_test, SMILES_test, y_test0 = pickle.load(f)\n",
    "    \n",
    "# with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\trainingsets\\train_regular_pubqc130K\\tokenizer.pickle', 'rb') as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "# tokenizer[0] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4.1\\Project\\DataProcessing\\CanonicalSmiles1.pickle', 'rb') as f:\n",
    "       X, SMILES, Y = pickle.load(f)\n",
    "\n",
    "with open(r'D:\\Studies\\PHD\\Research\\Study\\Study 4.1\\Project\\DataProcessing\\newTokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "        \n",
    "        \n",
    "print(X.shape)\n",
    "X = X.reshape(208742, 50, 35, 1)\n",
    "X.shape\n",
    "\n",
    "Y = Y.astype(float)\n",
    "X = X.astype(float)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1015ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = [len(smiles) for smiles in SMILES]\n",
    "# lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Given SMILES array and previously computed lengths array...\n",
    "\n",
    "# # Plotting the density of lengths\n",
    "# sns.kdeplot(lengths, shade=True)\n",
    "# plt.xlabel('SMILES Length')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Density of SMILES Lengths')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "X_smiles_train, X_smiles_test, y_train0, y_test0, SMILES_train, SMILES_test = train_test_split(X, Y, SMILES , test_size=1 - train_ratio)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "\n",
    "#x_val, x_test, y_val, y_test, SMILES_train, SMILES_test= train_test_split(x_test, y_test, SMILES_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(X_smiles_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_smiles_train.shape)\n",
    "print(X_smiles_train[100000,39,26,0])\n",
    "print(y_train0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting numerical values\n",
    "# values = y_train0\n",
    "# values1 = y_test0\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.kdeplot(values, fill=True, color='blue', label='Train')\n",
    "# sns.kdeplot(values1, fill=True, color='red', label='Test')\n",
    "# plt.title('Density Plot of Numerical Values')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Density')\n",
    "# plt.legend()\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_smiles_train0, SMILES_train0, y_train00 = X_smiles_train.copy(), SMILES_train.copy(), y_train0.copy()\n",
    "# X_smiles_test0, SMILES_test0, y_test_real = X_smiles_test.copy(), SMILES_test.copy(), y_test0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e647a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the gap value between [0-11]\n",
    "\n",
    "\n",
    "y_train0 = (y_train0 - 0.37) / 4.15\n",
    "y_test0 = (y_test0 - 0.37) / 4.15\n",
    "\n",
    "# print ('min norm', min(y_train))\n",
    "print ('min ', min(y_train0))\n",
    "# print ('max norm', max(y_train))\n",
    "print ('max ', max(y_train0))\n",
    "\n",
    "# print ('min norm',min(y_test))\n",
    "print ('min ',min(y_test0))\n",
    "# print ('max norm',max(y_test))\n",
    "print ('max ',max(y_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e59c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train0 = y_train.copy()\n",
    "# y_test0 = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding to an image embedding\n",
    "# ENCODER\n",
    "inp_1 = Input(shape = [50, 35, 1])\n",
    "\n",
    "y1 = Conv2D(64, (3, 1), strides = 1, padding = 'valid')(inp_1)\n",
    "y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "y1 = BatchNormalization()(y1)\n",
    "\n",
    "for _ in range(3): # Adding one more Conv layer\n",
    "    y1 = Conv2D(64, (3, 1), strides = 1, padding = 'valid')(y1)\n",
    "    y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "y1 = Conv2D(64, (17, 10), strides = 1, padding = 'valid')(y1)\n",
    "y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "y1 = BatchNormalization()(y1)\n",
    "\n",
    "y1 = Conv2D(64, 7, strides = 1, padding = 'valid')(y1)\n",
    "y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "y1 = BatchNormalization()(y1)\n",
    "\n",
    "y1 = Conv2D(64, 5, strides = 1, padding = 'valid')(y1)\n",
    "y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "y1 = BatchNormalization()(y1)\n",
    "\n",
    "for _ in range(2):\n",
    "    y1 = Conv2D(64, 4, strides = 1, padding = 'valid')(y1)\n",
    "    y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "for _ in range(2):    \n",
    "    y1 = Conv2D(64, 3, strides = 1, padding = 'valid')(y1)\n",
    "    y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "y1_emb = Conv2D(1, 3, strides = 1, padding = 'same',\n",
    "            activation = 'tanh')(y1)\n",
    "\n",
    "y2 = Conv2D(64, (3, 1), strides = 1, padding = 'valid')(inp_1)\n",
    "y2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "y2 = BatchNormalization()(y2)\n",
    "\n",
    "for _ in range(3): # Adding one more Conv layer\n",
    "    y2 = Conv2D(64, (3, 1), strides = 1, padding = 'valid')(y2)\n",
    "    y2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "\n",
    "y2 = Conv2D(64, (17, 10), strides = 1, padding = 'valid')(y2)\n",
    "y2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "y2 = BatchNormalization()(y2)\n",
    "\n",
    "y2 = Conv2D(64, 7, strides = 1, padding = 'valid')(y2)\n",
    "y2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "y2 = BatchNormalization()(y2)\n",
    "\n",
    "y2 = Conv2D(64, 5, strides = 1, padding = 'valid')(y2)\n",
    "y2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "y2 = BatchNormalization()(y2)\n",
    "\n",
    "for _ in range(2):\n",
    "    y2 = Conv2D(64, 4, strides = 1, padding = 'valid')(y2)\n",
    "    y2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    \n",
    "for _ in range(2):\n",
    "    y2 = Conv2D(64, 3, strides = 1, padding = 'valid')(y2)\n",
    "    y2 = LeakyReLU(alpha = 0.2)(y2)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "\n",
    "y2_emb = Conv2D(1, 3, strides = 1, padding = 'same',\n",
    "                activation = 'tanh')(y2)\n",
    "\n",
    "####\n",
    "y_out = Concatenate()([y1_emb, y2_emb])\n",
    "\n",
    "# DECODER\n",
    "emb_in = Input(shape = [6, 6, 2])\n",
    "\n",
    "tower0 = Conv2D(32, 1, padding = 'same')(emb_in)\n",
    "tower1 = Conv2D(64, 1, padding = 'same')(emb_in)\n",
    "tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "tower2 = Conv2D(32, 1, padding = 'same')(emb_in)\n",
    "tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "tower3 = MaxPooling2D(3, 1, padding = 'same')(emb_in)\n",
    "tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "h = ReLU()(h)\n",
    "h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "\n",
    "for i in range(6):\n",
    "    tower0 = Conv2D(32, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 1, padding = 'same')(h)\n",
    "    tower1 = Conv2D(64, 3, padding = 'same')(tower1)\n",
    "    tower2 = Conv2D(32, 1, padding = 'same')(h)\n",
    "    tower2 = Conv2D(32, 5, padding = 'same')(tower2)\n",
    "    tower3 = MaxPooling2D(3, 1, padding = 'same')(h)\n",
    "    tower3 = Conv2D(32, 1, padding = 'same')(tower3)\n",
    "    h = Concatenate()([tower0, tower1, tower2, tower3])\n",
    "    h = ReLU()(h)\n",
    "    if i % 2 == 0 and i != 0:\n",
    "        h = MaxPooling2D(2, 1, padding = 'same')(h)\n",
    "h = BatchNormalization()(h)\n",
    "\n",
    "y = Flatten()(h)\n",
    "y = Dense(4096, activation = 'relu')(y)\n",
    "y_cv = Dense(64, activation = 'relu')(y)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(2048, activation = 'relu')(y)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(1024, activation = 'sigmoid')(y)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(50 * 35)(y)\n",
    "y = Reshape([50, 35, 1])(y)\n",
    "y = Softmax(axis = 2)(y)\n",
    "\n",
    "y_cv = Dropout(0.2)(y_cv)\n",
    "y_cv = Dense(128, activation = 'relu')(y_cv)\n",
    "y_cv = Dropout(0.2)(y_cv)\n",
    "y_cv = Dense(128, activation = 'relu')(y_cv)\n",
    "y_cv = Dense(1, activation = 'sigmoid')(y_cv)\n",
    "\n",
    "encoder = Model([inp_1], [y1_emb, y2_emb, y_out], name = 'Encoder')\n",
    "decoder = Model(emb_in, [y, y_cv], name = 'Decoder')\n",
    "outputs = decoder(encoder([inp_1])[2])\n",
    "model = Model(inp_1, outputs, name = 'ae')\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28324437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre trained models\n",
    "# encoder = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\nns3\\ep5\\encoder.h5')\n",
    "# decoder = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\nns3\\ep5\\decoder.h5')\n",
    "# model = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\nns3\\ep5\\ae_model.h5')\n",
    "# 0.01 gets the best accuracy\n",
    "                   \n",
    "#encoder = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\result 2\\result 2\\encoder.h5')\n",
    "#decoder = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\RRCGAN_Molecules_Ehl-main\\data\\result 2\\result 2\\decoder.h5')\n",
    "#model = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4.1\\Experiments\\Regular\\Results 2\\Results 2\\ae_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "#encoder = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\Kia\\RRCGAN_Molecules_Ehl-main1\\RRCGAN_Molecules_Ehl-main\\data\\nns\\keep\\encoder.h5')\n",
    "#decoder = load_model(r'D:\\Studies\\PHD\\Research\\Study\\Study 4\\Kia\\RRCGAN_Molecules_Ehl-main1\\RRCGAN_Molecules_Ehl-main\\data\\nns\\keep\\decoder.h5')\n",
    "\n",
    "# model = tf.keras.models.load_model('./../data/nns2/ae_model2.h5')\n",
    "# encoder = tf.keras.models.load_model('./../data/nns2/encoder2.h5')\n",
    "# decoder = tf.keras.models.load_model('./../data/nns2/decoder2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26832ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate = 1e-6), \n",
    "              loss = ['binary_crossentropy', 'mse'], loss_weights = [1.0, 1.0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869a9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_smiles_train, [X_smiles_train, y_train0],  validation_data = (X_smiles_test, [X_smiles_test, y_test0]),\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    verbose=1)\n",
    "\n",
    "model.save  ('ae_model.h5')\n",
    "encoder.save('encoder.h5')\n",
    "decoder.save('decoder.h5')\n",
    "\n",
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4badf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.close()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.title('Autoencoder loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.tight_layout()\n",
    "plt.savefig('aeloss.jpeg', dpi=400)\n",
    "\n",
    "# get i and i+2 to have (2,9,10,1) shape\n",
    "# if only i was chosen, the should be (9,10,1)\n",
    "output = decoder.predict(encoder.predict([X_smiles_train[0:2][:][:][:]])[2])[0][0]\n",
    "output = argmax (output, axis=1)\n",
    "output = to_categorical (output, num_classes = 19)\n",
    "print (SMILES_train[0])\n",
    "print (output.shape)\n",
    "print ('output of decoder', output)\n",
    "print (y_train[0])\n",
    "print (SMILES_test.shape)\n",
    "with open ('aeloss.csv', 'w') as f:\n",
    "    for key in history.history.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,history.history[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3417b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_smiles_test[200].shape)\n",
    "plt.imshow(X_smiles_test[200].reshape([50, 35]), )\n",
    "plt.title(SMILES_test[200], fontfamily='Arial', fontsize=15, fontweight='bold', pad=30)\n",
    "plt.xlabel('Char in SMILES (27)', fontfamily='Arial', fontsize=15, fontweight='bold'), \n",
    "plt.ylabel('Length of SMILES string', fontfamily='Arial', fontsize=15, fontweight='bold')\n",
    "plt.xticks((0, 10, 20, 26), size=15)\n",
    "plt.yticks((0, 10, 20, 30, 39), size=15)\n",
    "plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_smiles_{}'.format(SMILES_test[200]), dpi=500)\n",
    "print (y_test0[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee795f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [5, 10, 32, 88, 99]:\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(X_smiles_test[i].reshape([50, 35]))\n",
    "    test_sample_pred = decoder.predict(encoder.predict([X_smiles_test[i:(i+2)]], verbose=0)[2], verbose=0)[0][0]\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(test_sample_pred.reshape([50, 35]))\n",
    "    #plt.xlabel('Char in SMILES (27)', fontfamily='Arial', fontsize=15, fontweight='bold'), \n",
    "    #plt.ylabel('Length of SMILES string', fontfamily='Arial', fontsize=15, fontweight='bold')\n",
    "    #plt.xticks((0, 10, 20, 26), size=15)\n",
    "    #plt.yticks((0, 10, 20, 30, 39), size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"smiles_{}_test.png\".format(i), dpi=500)\n",
    "    print (i, SMILES_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 100, 320, 880, 990]:\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(X_smiles_test[i].reshape([50, 35]))\n",
    "    test_sample_pred = decoder.predict(encoder.predict([X_smiles_test[i:(i+2)]], verbose=0)[2], verbose=0)[0][0]\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(test_sample_pred.reshape([50, 35]))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"smiles_{}_test.png\".format(i), dpi=500)\n",
    "    print (i, SMILES_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: test data\n",
    "# 0.8113100848256362 valid and true\n",
    "# 0.8619792648444863  valid\n",
    "encoder_latent_test = encoder.predict(X_smiles_test, verbose=0)[2]\n",
    "decoder_smiles_test = decoder.predict(encoder_latent_test, verbose=0)[0]\n",
    "\n",
    "dec_SMILES_test = []\n",
    "for softmax_smiles in decoder_smiles_test:\n",
    "    argmax_smiles = np.argmax(softmax_smiles, axis = 1).reshape([-1])\n",
    "    #print (argmax_smiles)\n",
    "    smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "    SHAPE = [1] + list(smiles.shape) + [1]\n",
    "    smiles = smiles.reshape(SHAPE)\n",
    "    c_smiles = ''\n",
    "    for s in argmax_smiles:\n",
    "        c_smiles += tokenizer[s]\n",
    "        c_smiles = c_smiles.rstrip()\n",
    "    dec_SMILES_test.append(c_smiles)\n",
    "    \n",
    "test_true_conv = sum (SMILES_test == dec_SMILES_test)/len(SMILES_test)\n",
    "print (test_true_conv)\n",
    "\n",
    "idxs_test = []\n",
    "valid_test = 0\n",
    "\n",
    "for count, smile in enumerate(dec_SMILES_test):\n",
    "    m = Chem.MolFromSmiles(smile[:-1], sanitize=True)\n",
    "    if m is not None:\n",
    "        valid_test += 1\n",
    "        idxs_test.append(count)\n",
    "\n",
    "test_valid_conv = valid_test/len(SMILES_test)\n",
    "print (test_valid_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: train data\n",
    "# 0.8113100848256362 valid and true\n",
    "# 0.8619792648444863  valid\n",
    "encoder_latent_train = encoder.predict(X_smiles_train, verbose=0)[2]\n",
    "decoder_smiles_train = decoder.predict(encoder_latent_train, verbose=0)[0]\n",
    "\n",
    "dec_SMILES_train = []\n",
    "for softmax_smiles in decoder_smiles_train:\n",
    "    argmax_smiles = np.argmax(softmax_smiles, axis = 1).reshape([-1])\n",
    "    #print (argmax_smiles)\n",
    "    smiles = to_categorical(argmax_smiles, num_classes=27)\n",
    "    SHAPE = [1] + list(smiles.shape) + [1]\n",
    "    smiles = smiles.reshape(SHAPE)\n",
    "    c_smiles = ''\n",
    "    for s in argmax_smiles:\n",
    "        c_smiles += tokenizer[s]\n",
    "        c_smiles = c_smiles.rstrip()\n",
    "    dec_SMILES_train.append(c_smiles)\n",
    "    \n",
    "train_true_conv = sum (SMILES_train == dec_SMILES_train)/len(SMILES_train)\n",
    "print (train_true_conv)\n",
    "\n",
    "idxs_train = []\n",
    "valid_train = 0\n",
    "\n",
    "for count, smile in enumerate(dec_SMILES_train):\n",
    "    m = Chem.MolFromSmiles(smile[:-1], sanitize=True)\n",
    "    if m is not None:\n",
    "        valid_train += 1\n",
    "        idxs_train.append(count)\n",
    "\n",
    "train_valid_conv = valid_train/len(SMILES_train)\n",
    "print (train_valid_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa1840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all data evaluation\n",
    "# 77.574 valid \n",
    "# 71.1124 valid and accurate\n",
    "valid_conv = (4*train_valid_conv + test_valid_conv)/5\n",
    "print ('Valid conversion: ', valid_conv)\n",
    "\n",
    "true_conv = (4*train_true_conv + test_true_conv)/5\n",
    "print ('Valid and True conversion: ', true_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_letters(a,b):\n",
    "    return sum ( a[i] != b[i] for i in range(len(a)) )\n",
    "# Evaluation: test data\n",
    "# 98.01629884592023 categorical accuracy\n",
    "encoder_latent_test = encoder.predict(X_smiles_test, verbose=0)[2]\n",
    "decoder_smiles_test = decoder.predict(encoder_latent_test, verbose=0)[0]\n",
    "\n",
    "dec_SMILES_test = []\n",
    "for softmax_smiles in decoder_smiles_test:\n",
    "    argmax_smiles = np.argmax(softmax_smiles, axis = 1).reshape([-1])\n",
    "    #print (argmax_smiles)\n",
    "    smiles = to_categorical(argmax_smiles, num_classes=35)\n",
    "    SHAPE = [1] + list(smiles.shape) + [1]\n",
    "    smiles = smiles.reshape(SHAPE)\n",
    "    c_smiles = ''\n",
    "    for s in argmax_smiles:\n",
    "        c_smiles += tokenizer[s]\n",
    "        c_smiles = c_smiles.rstrip()\n",
    "    dec_SMILES_test.append(c_smiles)\n",
    "\n",
    "wrong_char = 0\n",
    "total_char = 0\n",
    "for i, j in zip(dec_SMILES_test, SMILES_test):\n",
    "    try:\n",
    "        wrong_char = wrong_char + diff_letters(i, j)\n",
    "    except:\n",
    "        print(i, j)\n",
    "    total_char = total_char + len(i) \n",
    "print (wrong_char)\n",
    "print ('% of correct char conv train: ', 100-wrong_char/total_char*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d40fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: train data\n",
    "#  99.98139237365801 categorical accuracy\n",
    "encoder_latent_train = encoder.predict(X_smiles_train, verbose=0)[2]\n",
    "decoder_smiles_train = decoder.predict(encoder_latent_train, verbose=0)[0]\n",
    "\n",
    "dec_SMILES_train = []\n",
    "for softmax_smiles in decoder_smiles_train:\n",
    "    argmax_smiles = np.argmax(softmax_smiles, axis = 1).reshape([-1])\n",
    "    #print (argmax_smiles)\n",
    "    smiles = to_categorical(argmax_smiles, num_classes=35)\n",
    "    SHAPE = [1] + list(smiles.shape) + [1]\n",
    "    smiles = smiles.reshape(SHAPE)\n",
    "    c_smiles = ''\n",
    "    for s in argmax_smiles:\n",
    "        c_smiles += tokenizer[s]\n",
    "        c_smiles = c_smiles.rstrip()\n",
    "    dec_SMILES_train.append(c_smiles)\n",
    "\n",
    "wrong_char = 0\n",
    "total_char = 0\n",
    "for i, j in zip(dec_SMILES_train, SMILES_train):\n",
    "    try:\n",
    "        wrong_char = wrong_char + diff_letters(i, j)\n",
    "    except:\n",
    "        print(i, j)\n",
    "    total_char = total_char + len(i) \n",
    "print ('% of correct char conv train: ', 100-wrong_char/total_char*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909e363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4765354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
